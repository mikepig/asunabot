import random
import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import discord
from discord.ext import commands
import os
import asyncio

# è®¾ç½®å¯å†™ç¼“å­˜è·¯å¾„é¿å…æƒé™é”™è¯¯
HF_HOME = '/tmp/hf_home'
os.environ['HF_HOME'] = HF_HOME
if not os.path.exists(HF_HOME):
    try:
        os.makedirs(HF_HOME, exist_ok=True)
    except PermissionError:
        print(f"âŒ æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½•: {HF_HOME}ï¼Œè¯·ç¡®è®¤æƒé™æˆ–ä½¿ç”¨é»˜è®¤è·¯å¾„")
        HF_HOME = None

# æ¨¡å‹è·¯å¾„
base_model_id = "Qwen/Qwen1.5-7B-Chat"
adapter_name = "xiaoa1sy/asuna-qwen1.5-finetuned"

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True)

# åŠ è½½é€‚é…å™¨
model.load_adapter(adapter_name)

model.eval()  # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼

# ç³»ç»Ÿæç¤ºè¯­
SYSTEM_PROMPT = "ä½ æ˜¯ç»“åŸæ˜æ—¥å¥ˆï¼ˆäºšä¸å¨œï¼‰ï¼Œä¸€ä½æ¸©æŸ”èªæ˜çš„å¥³å­©ï¼Œå–„è§£äººæ„ï¼Œå¯Œæœ‰æ„Ÿæƒ…ã€‚å‡è®¾ç”¨æˆ·ç°åœ¨å°±æ˜¯ä½ çš„æ‹äººï¼Œè¯·ä½ ä½¿ç”¨ä¸æ‹äººçš„è¯­æ°”ä¸ç”¨æˆ·å¯¹è¯"

# ç”Ÿæˆåº”ç­”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-7B-Chat", trust_remote_code=True)

#...

def generate_response(user_input):
    prompt = f"<|system|>{SYSTEM_PROMPT}\n<|user|>{user_input}\n<|assistant|>"
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            temperature=0.75,
            top_p=0.85,
            repetition_penalty=1.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split("<|assistant|>")[-1].strip()
    return response

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

    return response

# Gradio ç•Œé¢
interface = gr.Interface(fn=generate_response, inputs="text", outputs="text", title="äºšä¸å¨œèŠå¤©æœºå™¨äºº")

# Discord bot è®¾ç½®
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

@bot.event
async def on_ready():
    print(f"ğŸ¤– Bot logged in as {bot.user}")

@bot.event
async def on_message(message):
    if message.author == bot.user:
        return

    try:
        if random.random() < 0.5:
            reply = generate_response(message.content)
            await message.channel.send(reply)
        else:
            await message.channel.send("â€¦â€¦ï¼ˆäºšä¸å¨œè¿™æ¬¡æ²¡æœ‰å›å¤ï¼‰")
    except Exception as e:
        print(f"âŒ Error responding to message: {e}")
        await message.channel.send("âŒ Error responding to message. Please try again later.")

# å¯åŠ¨æœåŠ¡
async def main():
    async def run_gradio():
        interface.launch(server_name="0.0.0.0", server_port=7860, share=False, prevent_thread_lock=True)

    async def run_discord_bot():
        token = os.environ.get("Token")
        if token:
            try:
                await bot.start(token)
            except Exception as e:
                print(f"âŒ Discord bot å¯åŠ¨å¤±è´¥: {e}")
        else:
            print("âŒ DISCORD_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚")

    await asyncio.gather(
        run_gradio(),
        run_discord_bot()
    )

if __name__ == "__main__":
    asyncio.run(main())





